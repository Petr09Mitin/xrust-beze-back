from fastapi import FastAPI, HTTPException
from fastapi import FastAPI
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import List
from pathlib import Path
from tqdm import tqdm
import os
import numpy as np
import faiss


import boto3
from tempfile import TemporaryDirectory
from langchain_community.docstore.in_memory import InMemoryDocstore

from pymongo import MongoClient
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS

from threading import Lock
import threading
import time

import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,  # –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(levelname)s - %(message)s',  # –§–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
    datefmt='%Y-%m-%d %H:%M:%S'  # –§–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏
)



FAISS_REBUILD_INTERVAL = 300  # —Å–µ–∫—É–Ω–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 5 –º–∏–Ω—É—Ç)


S3_BUCKET = "materials"
S3_PREFIX = "materials"

S3_CONFIG = {
    'aws_access_key_id': os.getenv("MINIO_ROOT_USER"),
    'aws_secret_access_key': os.getenv("MINIO_ROOT_PASSWORD"),
    'endpoint_url': os.getenv("S3_ENDPOINT_URL")  # –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π URL –¥–ª—è S3-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
}

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
INDEX_DIR = Path("./vectorstore/faiss_index")
EMBEDDING_MODEL_NAME = "intfloat/e5-base-v2"


MONGO_URI = os.getenv("MONGO_DB_URL")
MONGO_DB = os.getenv("MONGO_DB")
MONGO_COLLECTION = os.getenv("MONGO_COLLECTION")

CHUNK_SIZE = 800
CHUNK_OVERLAP = 50

# --- FastAPI ---

@asynccontextmanager
async def lifespan(app: FastAPI):
    init_vectorstore()
    start_faiss_rebuilder()
    yield  # –∑–¥–µ—Å—å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Å–µ—Ä–≤–µ—Ä
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏

app = FastAPI(lifespan=lifespan, title="RAG FastAPI with MongoDB")


# --- MongoDB ---
mongo_client = MongoClient(MONGO_URI)
db = mongo_client[MONGO_DB]
collection = db[MONGO_COLLECTION]

# --- –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ---
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS ---
vectorstore = None


# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ---
def clean_text(text: str) -> str:
    text = text.replace("-\n", "").replace("\n", " ")
    return text


def sync_mongo_with_s3(bucket: str, prefix: str, s3_config: dict) -> bool:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç Mongo –∏ S3:
    - –£–¥–∞–ª—è–µ—Ç –∏–∑ Mongo —á–∞–Ω–∫–∏, –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –∏—Å—á–µ–∑ –∏–∑ S3
    - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –±—ã–ª–∏ –ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    """
    s3 = boto3.client("s3", **s3_config)

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ S3
    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    s3_keys = set()
    for obj in response.get("Contents", []):
        key = obj["Key"]
        if not key.endswith("/"):
            s3_keys.add(key)

    # 2. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö source –∏–∑ Mongo
    mongo_sources = set(collection.distinct("metadata.source"))

    # 3. –£–¥–∞–ª—è–µ–º –∏–∑ Mongo –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ S3
    to_delete = mongo_sources - s3_keys
    # print(f'To delete: {to_delete}')
    logging.debug(f'To delete: {to_delete}')
    updated = False

    for missing_file in to_delete:
        logging.debug(f"[CLEANUP] –£–¥–∞–ª—è—é —á–∞–Ω–∫–∏ –∏–∑ Mongo –¥–ª—è: {missing_file}")
        # print(f"[CLEANUP] –£–¥–∞–ª—è—é —á–∞–Ω–∫–∏ –∏–∑ Mongo –¥–ª—è: {missing_file}")
        collection.delete_many({"metadata.source": missing_file})
        updated = True

    # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã (–ª–µ–Ω–∏–≤–æ, –ø–æ –æ–¥–Ω–æ–º—É)
    with TemporaryDirectory() as temp_dir:
        for key in s3_keys:
            if key in mongo_sources:
                logging.debug(f"[SKIP] –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {key}")
                # print(f"[SKIP] –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {key}")
                continue
            
            logging.info(f"[PROCESS] –ù–æ–≤—ã–π —Ñ–∞–π–ª: {key}")
            # print(f"[PROCESS] –ù–æ–≤—ã–π —Ñ–∞–π–ª: {key}")
            local_path = os.path.join(temp_dir, os.path.basename(key))
            s3.download_file(bucket, key, local_path)

            if key.lower().endswith(".pdf"):
                loader = PyPDFLoader(local_path)
            elif key.lower().endswith(".txt"):
                loader = TextLoader(local_path)
            else:
                logging.warning(f"[SKIP] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {key}")
                # print(f"[SKIP] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {key}")
                continue

            docs = []
            for doc in loader.load():
                doc.page_content = clean_text(doc.page_content)
                doc.metadata["source"] = key
                docs.append(doc)

            chunks = split_documents(docs)
            compute_and_store_embeddings(chunks)
            updated = True

    return updated


def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    return splitter.split_documents(documents)


def compute_and_store_embeddings(chunks: List[Document]):
    embeddings = []
    new_chunks = []
    for chunk in tqdm(chunks, desc="Embedding"):
        text = chunk.page_content
        metadata = chunk.metadata
        if collection.find_one({"text": text}):
            continue  # —É–∂–µ –µ—Å—Ç—å
        try:
            vector = embedding_model.embed_query(text)
            doc = {
                "text": text,
                "embedding": vector,
                "metadata": metadata,
            }
            collection.insert_one(doc)
            embeddings.append(vector)
            new_chunks.append(chunk)
        except:
            return None
    return embeddings, new_chunks


def build_faiss_from_mongo():
    logging.info("[FAISS] –ß—Ç–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ MongoDB...")
    # print("[INFO] –ß—Ç–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ MongoDB...")
    embeddings = []
    documents = []

    for i, doc in enumerate(collection.find({}, {"embedding": 1, "text": 1, "metadata": 1})):
        vector = doc["embedding"]
        text = doc["text"]
        metadata = doc.get("metadata", {})
        document = Document(page_content=text, metadata=metadata)
        documents.append(document)
        embeddings.append(vector)

    logging.info("[FAISS] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ ...")
    # print("[INFO] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –≤—Ä—É—á–Ω—É—é...")
    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings).astype("float32"))

    docstore = InMemoryDocstore({str(i): documents[i] for i in range(len(documents))})
    index_to_docstore_id = {i: str(i) for i in range(len(documents))}

    return FAISS(
        embedding_function=embedding_model,
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id
    )


def init_vectorstore():
    global vectorstore
    logging.info("[INFO] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è Mongo <-> S3")
    # print("[INFO] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è Mongo <-> S3")
    updated = sync_mongo_with_s3(S3_BUCKET, S3_PREFIX, S3_CONFIG)

    if INDEX_DIR.exists() and not updated:
        logging.info("[INFO] –ó–∞–≥—Ä—É–∂–∞—é FAISS –∏–∑ –¥–∏—Å–∫–∞ (–∏–Ω–¥–µ–∫—Å –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è)...")
        # print("[INFO] –ó–∞–≥—Ä—É–∂–∞—é FAISS –∏–∑ –¥–∏—Å–∫–∞ (–∏–Ω–¥–µ–∫—Å –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è)...")
        vectorstore = FAISS.load_local(str(INDEX_DIR), embeddings=embedding_model, allow_dangerous_deserialization=True)
    else:
        logging.info("[INFO] –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞—é FAISS –∏–∑ Mongo...")
        # print("[INFO] –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞—é FAISS –∏–∑ Mongo...")
        vectorstore = build_faiss_from_mongo()
        vectorstore.save_local(str(INDEX_DIR))

    logging.info("[INFO] –ò–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤.")
    # print("[INFO] –ò–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤.")



def build_prompt(query: str, docs: List[Document]) -> str:
    parts = []
    for i, doc in enumerate(docs):
        # print(doc.metadata.keys())  # ‚Üê –ø–æ–∫–∞–∂–µ—Ç —Ç–æ—á–Ω—ã–µ –∫–ª—é—á–∏
        # print(doc.metadata)         # ‚Üê –ø–æ–∫–∞–∂–µ—Ç –≤—Å—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        source = doc.metadata.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        parts.append(f"[{i+1}] –ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n{doc.page_content}")
    context = "\n\n".join(parts)
    return f"""–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å:
{query}

–û—Ç–≤–µ—Ç:"""


# --- –ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞ ---
class QueryRequest(BaseModel):
    query: str
    k: int = 3


class NotifyRequest(BaseModel):
    key: str  # S3 –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É


def extraction_data(docs):

    result = {
        'list_docs': []
    }
    for doc in docs:
        source = doc.metadata.get("source", "invalid")
        if source == "invalid":
            continue

        text = doc.page_content
        result['list_docs'].append({
            'doc_name': source,
            'text_fragment': text
        })
    return result


# --- Endpoint ---
@app.post("/query")
def query_rag(request: QueryRequest):
    with vectorstore_lock:
        vs = vectorstore

    docs = vs.similarity_search(request.query, k=request.k)
    result = extraction_data(docs)
    # prompt = build_prompt(request.query, docs)
    return result


@app.post("/delete")
def delete_file_from_db(request: NotifyRequest):
    # to_delete = request.key
    # # for missing_file in to_delete:
    # #     print(f"[CLEANUP] –£–¥–∞–ª—è—é —á–∞–Ω–∫–∏ –∏–∑ Mongo –¥–ª—è: {missing_file}")
    try:
        collection.delete_many({"metadata.source": request.key})
        logging.info(f"[DELETED]: {request.key}")
        # print(f"[DELETED]: {request.key}")
    except HTTPException as e:
        logging.error(e)
        # print(e)
        raise HTTPException(status_code=500, detail="Can't delete file")
        # updated = True


@app.post("/add")
def notify_new_document(request: NotifyRequest):
    key = request.key

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Ñ–∞–π–ª –≤ Mongo
    if collection.count_documents({"metadata.source": key}) > 0:
        return {"status": "skipped", "message": f"{key} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω."}

    logging.info(f"[NOTIFY] –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {key}")
    # print(f"[NOTIFY] –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {key}")

    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
    with TemporaryDirectory() as temp_dir:
        local_path = os.path.join(temp_dir, os.path.basename(key))

        s3 = boto3.client("s3", **S3_CONFIG)
        try:
            s3.download_file(S3_BUCKET, key, local_path)
        except Exception as e:
            raise HTTPException(status_code=404, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {key}: {str(e)}")

        # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
        if key.lower().endswith(".pdf"):
            loader = PyPDFLoader(local_path)
        elif key.lower().endswith(".txt"):
            loader = TextLoader(local_path)
        else:
            raise HTTPException(status_code=400, detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {key}")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        docs = []
        for doc in loader.load():
            doc.page_content = clean_text(doc.page_content)
            doc.metadata["source"] = key
            docs.append(doc)

        chunks = split_documents(docs)
        compute_and_store_embeddings(chunks)

    # # –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏–º FAISS (–º–æ–∂–Ω–æ ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    # global vectorstore
    # vectorstore = build_faiss_from_mongo()
    # vectorstore.save_local(str(INDEX_DIR))

    return {"status": "success", "message": f"{key} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É"}







vectorstore_lock = Lock()

def start_faiss_rebuilder():
    # print("[FAISS] üîÅ –ü–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω")
    # time.sleep(FAISS_REBUILD_INTERVAL)

    def background_rebuild():
        logging.info("[FAISS] üîÅ –ü–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω")
        # print("[FAISS] üîÅ –ü–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω")
        time.sleep(FAISS_REBUILD_INTERVAL)
        global vectorstore
        while True:
            logging.info("[FAISS] ‚è≥ –ü–ª–∞–Ω–æ–≤–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ Mongo...")
            # print("[FAISS] ‚è≥ –ü–ª–∞–Ω–æ–≤–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ Mongo...")
            new_vs = build_faiss_from_mongo()
            new_vs.save_local(str(INDEX_DIR))

            with vectorstore_lock:
                vectorstore = new_vs  # –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–º–µ–Ω–∞

            logging.info("[FAISS] ‚úÖ –ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω")
            # print("[FAISS] ‚úÖ –ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω")
            time.sleep(FAISS_REBUILD_INTERVAL)

            # time.sleep(FAISS_REBUILD_INTERVAL)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ –∫–∞–∫ daemon
    thread = threading.Thread(target=background_rebuild, daemon=True)
    thread.start()




